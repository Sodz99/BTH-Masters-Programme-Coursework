{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises - Neural Machine Translation\n",
    "\n",
    "This lab focuses on implementing three approaches for neural machine translation that progressively make better translation quality. The implementations should use German-to-English translation with the Multi30k dataset, which has ~30,000 parallel English and German sentences [Check here](https://github.com/multi30k/dataset/tree/master/data/task1/raw).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "Implement the foundational encoder-decoder architecture using LSTM networks. Build a core system where an encoder reads an entire German sentence and compresses all information into a single context vector, then a decoder uses this vector to generate the English translation word by word.\n",
    "\n",
    "\n",
    "**Implementation Tasks:**\n",
    "\n",
    "1. **Preprocessing**\n",
    "   - Load and tokenize the Multi30k German-English dataset\n",
    "   - Build vocabulary dictionaries for both languages with special tokens (<sos>, <eos>, <unk>, <pad>)\n",
    "   - Implement data loaders with proper padding and batching\n",
    "\n",
    "2. **Encoder**\n",
    "   - Create a 2-layer LSTM encoder that processes German sentences\n",
    "   - Implement embedding layers for input tokens\n",
    "   - Add dropout layers for regularization\n",
    "   - Extract final hidden and cell states as context vectors\n",
    "\n",
    "3. **Decoder**\n",
    "   - Build a 2-layer LSTM decoder that generates English translations\n",
    "   - Implement teacher forcing mechanism for training\n",
    "   - Add linear output layer to predict vocabulary probabilities\n",
    "   - Handle variable-length sequence generation\n",
    "\n",
    "4. **Training & Evaluation**\n",
    "   - Implement training loop with proper loss calculation (CrossEntropyLoss)\n",
    "   - Add gradient clipping to prevent exploding gradients\n",
    "   - Evaluate model performance using BLEU score\n",
    "   - Save best model based on validation loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Phrase Representations using RNN Encoder-Decoder\n",
    "\n",
    "Build upon the first approach with improvements using GRU (Gated Recurrent Unit) networks instead of LSTMs for better computational efficiency. make improvements on how the decoder uses context information by providing access to both the previous hidden state and the context vector at each decoding step.\n",
    "\n",
    "**Implementation Tasks:**\n",
    "\n",
    "1. **Architecture Design**\n",
    "   - Replace LSTM with single-layer GRU networks in both encoder and decoder\n",
    "   - Modify decoder to concatenate embeddings with context vector at each time step\n",
    "   - Implement enhanced linear output layer that uses embeddings, hidden state, and context\n",
    "\n",
    "2. **Context Vector**\n",
    "   - Ensure context vector is passed to decoder at every time step (not just initialization)\n",
    "   - Modify decoder forward pass to accept three inputs: token embedding, hidden state, and context\n",
    "   - Implement proper tensor concatenation for enhanced feature representation\n",
    "\n",
    "3. **Weight Initialization**\n",
    "   - Initialize all parameters using normal distribution (mean=0, std=0.01)\n",
    "   - Ensure consistent initialization across all model components\n",
    "\n",
    "4. **Analysis**\n",
    "   - Compare training time and memory usage with LSTM implementation\n",
    "   - Analyze BLEU score improvements over baseline seq2seq model\n",
    "   - Generate sample translations and compare quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "Implement attention mechanisms to address the context vector bottleneck. The model learns to focus on different parts of the source sentence when generating each word of the translation. Include bidirectional encoding to better capture context from both directions.\n",
    "\n",
    "**Implementation Tasks:**\n",
    "\n",
    "1. **Bidirectional Encoder**\n",
    "   - Implement bidirectional GRU that reads source sentence in both directions\n",
    "   - Concatenate forward and backward hidden states for richer representations\n",
    "   - Add linear transformation layer to project concatenated states to decoder dimension\n",
    "\n",
    "2. **Attention Mechanism**\n",
    "   - Create attention module that computes alignment scores between decoder hidden state and all encoder outputs\n",
    "   - Implement energy function using linear layers and tanh activation\n",
    "   - Apply softmax to get attention weights over source positions\n",
    "   - Compute weighted context vector using attention weights\n",
    "\n",
    "3. **Enhanced Decoder**\n",
    "   - Modify decoder to accept encoder outputs and compute attention at each step\n",
    "   - Concatenate current embedding, weighted context, and previous hidden state as RNN input\n",
    "   - Update output layer to use embedding, hidden state, and attended context for prediction\n",
    "\n",
    "4. **Attention Visualization**\n",
    "   - Implement function to extract and store attention weights during translation\n",
    "   - Create visualization plots showing attention alignment between source and target words\n",
    "   - Generate attention heatmaps for sample translations\n",
    "\n",
    "5. **Advanced Training**\n",
    "   - Implement proper weight initialization (normal for weights, zero for biases)\n",
    "   - Monitor both training loss and validation BLEU score\n",
    "   - Save attention weights for analysis and visualization\n",
    "\n",
    "6. **Evaluation**\n",
    "   - Evaluate final model on test set and report BLEU scores\n",
    "   - Compare translation quality across all three implementations\n",
    "   - Analyze attention patterns for different sentence types and lengths\n",
    "   - Generate qualitative analysis of translation improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
