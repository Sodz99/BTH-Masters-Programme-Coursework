{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "### Implementation Overview\n",
    "\n",
    "- This notebook represents a personal implementation of the seq2seq architecture as introduced by Sutskever et al. (2014) focusing on understanding core concepts and challenges in neural machine translation.\n",
    "\n",
    "- The approach addresses the sequence transformation problem of converting German sentences (variable length) into English sentences (different variable length) while maintaining semantic meaning throughout the process.\n",
    "\n",
    "- The implementation uses an encoder-decoder architecture where the encoder compresses source sentences into fixed-size representations and the decoder generates target sequences from these representations. The key insight involves using LSTM networks for both encoding and decoding phases with teacher forcing during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset - this provides German-English sentence pairs\n",
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "# Examining the structure to understand the data format\n",
    "print(\"Dataset structure:\", dataset)\n",
    "print(\"\\nFirst training example:\", dataset[\"train\"][0])\n",
    "\n",
    "# Splitting into train/validation/test\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"], \n",
    "    dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading language models for tokenization\n",
    "# These need to be downloaded first: python -m spacy download en_core_web_sm de_core_news_sm\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Testing the tokenizers to see how they work\n",
    "test_en = \"Hello, world! How are you?\"\n",
    "test_de = \"Hallo, Welt! Wie geht es dir?\"\n",
    "\n",
    "print(\"English tokens:\", [token.text for token in en_nlp.tokenizer(test_en)])\n",
    "print(\"German tokens:\", [token.text for token in de_nlp.tokenizer(test_de)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    \"\"\"\n",
    "    Preprocessing function - tokenize and prepare the data\n",
    "    \n",
    "    Key decisions made:\n",
    "    - Truncating to max_length to avoid memory issues\n",
    "    - Converting to lowercase for vocabulary reduction\n",
    "    - Adding special tokens for sequence boundaries\n",
    "    \"\"\"\n",
    "    # Tokenize both languages\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    \n",
    "    # Lowercase if specified - this is a trade-off between vocab size and information\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    \n",
    "    # Add sequence markers - essential for the model to understand boundaries\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    \n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n",
    "\n",
    "# Apply preprocessing to all data\n",
    "max_length = 1000  # Should be sufficient for image captions\n",
    "lower = True       # Reducing vocabulary complexity\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "preprocessing_args = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(preprocess_example, fn_kwargs=preprocessing_args)\n",
    "valid_data = valid_data.map(preprocess_example, fn_kwargs=preprocessing_args)\n",
    "test_data = test_data.map(preprocess_example, fn_kwargs=preprocessing_args)\n",
    "\n",
    "# Check the result\n",
    "print(\"Preprocessed example:\", train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabularies - converting tokens to indices\n",
    "min_freq = 2  # Ignore rare tokens to reduce noise\n",
    "unk_token = \"<unk>\"  # For unknown tokens\n",
    "pad_token = \"<pad>\"  # For padding sequences to same length\n",
    "\n",
    "special_tokens = [unk_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "print(f\"Building vocabularies with min_freq={min_freq}...\")\n",
    "\n",
    "# Build English vocabulary from training data only (avoid data leakage)\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "# Build German vocabulary\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "print(f\"English vocabulary size: {len(en_vocab)}\")\n",
    "print(f\"German vocabulary size: {len(de_vocab)}\")\n",
    "print(f\"Most common English tokens: {en_vocab.get_itos()[:10]}\")\n",
    "print(f\"Most common German tokens: {de_vocab.get_itos()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up unknown token handling\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "# These should be the same for both vocabularies due to special_tokens ordering\n",
    "assert en_vocab[unk_token] == de_vocab[unk_token], \"Vocab mismatch!\"\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token], \"Vocab mismatch!\"\n",
    "\n",
    "# Set default behavior for unknown tokens\n",
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)\n",
    "\n",
    "print(f\"Unknown token index: {unk_index}\")\n",
    "print(f\"Padding token index: {pad_index}\")\n",
    "\n",
    "# Test unknown token handling\n",
    "test_token = \"supercalifragilisticexpialidocious\"\n",
    "print(f\"Unknown token '{test_token}' maps to index: {en_vocab[test_token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indices(example, en_vocab, de_vocab):\n",
    "    \"\"\"\n",
    "    Convert token strings to vocabulary indices\n",
    "    This is the final step before the data can be fed to the model\n",
    "    \"\"\"\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n",
    "\n",
    "# Apply to all datasets\n",
    "vocab_args = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "train_data = train_data.map(convert_to_indices, fn_kwargs=vocab_args)\n",
    "valid_data = valid_data.map(convert_to_indices, fn_kwargs=vocab_args)\n",
    "test_data = test_data.map(convert_to_indices, fn_kwargs=vocab_args)\n",
    "\n",
    "# Convert to PyTorch tensors for efficiency\n",
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(type=data_type, columns=format_columns, output_all_columns=True)\n",
    "valid_data = valid_data.with_format(type=data_type, columns=format_columns, output_all_columns=True)\n",
    "test_data = test_data.with_format(type=data_type, columns=format_columns, output_all_columns=True)\n",
    "\n",
    "print(\"Converted example:\", train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collate_function(pad_index):\n",
    "    \"\"\"\n",
    "    Create a collation function for batching variable-length sequences\n",
    "    \n",
    "    My reasoning: Different sentences have different lengths, but neural networks\n",
    "    need fixed-size inputs. Solution: pad shorter sequences with pad_token.\n",
    "    \"\"\"\n",
    "    def collate_batch(batch):\n",
    "        # Extract English and German sequences\n",
    "        en_sequences = [example[\"en_ids\"] for example in batch]\n",
    "        de_sequences = [example[\"de_ids\"] for example in batch]\n",
    "        \n",
    "        # Pad sequences to the same length\n",
    "        en_padded = nn.utils.rnn.pad_sequence(en_sequences, padding_value=pad_index)\n",
    "        de_padded = nn.utils.rnn.pad_sequence(de_sequences, padding_value=pad_index)\n",
    "        \n",
    "        return {\n",
    "            \"en_ids\": en_padded,\n",
    "            \"de_ids\": de_padded,\n",
    "        }\n",
    "    \n",
    "    return collate_batch\n",
    "\n",
    "def create_dataloader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    \"\"\"\n",
    "    Create a DataLoader with proper collation\n",
    "    \"\"\"\n",
    "    collate_fn = create_collate_function(pad_index)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128  # Balance between memory usage and training efficiency\n",
    "\n",
    "train_loader = create_dataloader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_loader = create_dataloader(valid_data, batch_size, pad_index, shuffle=False)\n",
    "test_loader = create_dataloader(test_data, batch_size, pad_index, shuffle=False)\n",
    "\n",
    "print(f\"Created data loaders with batch size {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder implementation\n",
    "    \n",
    "    Purpose: Convert variable-length German sequence into fixed-size context vector\n",
    "    \n",
    "    Architectural decisions:\n",
    "    - Embedding layer: Convert token indices to dense vectors\n",
    "    - LSTM: Capture sequential patterns and long-range dependencies  \n",
    "    - Multi-layer: Increase model capacity for complex patterns\n",
    "    - Dropout: Prevent overfitting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store dimensions for later use\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Components\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, source_sequence):\n",
    "        \"\"\"\n",
    "        Forward pass logic:\n",
    "        1. Embed the token indices into dense vectors\n",
    "        2. Apply dropout for regularization\n",
    "        3. Pass through LSTM to get context vectors\n",
    "        4. Return final hidden and cell states as context\n",
    "        \"\"\"\n",
    "        # source_sequence shape: [seq_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(source_sequence))\n",
    "        # embedded shape: [seq_len, batch_size, embedding_dim]\n",
    "        \n",
    "        # LSTM returns: outputs, (final_hidden, final_cell)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Only the final states are needed as context for the decoder\n",
    "        return hidden, cell\n",
    "\n",
    "print(\"Encoder implementation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder implementation\n",
    "    \n",
    "    Purpose: Generate English words one at a time using context from encoder\n",
    "    \n",
    "    Key insight: This is an autoregressive model - each prediction depends on\n",
    "    previous predictions and the encoder context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Components\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_rate)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, input_token, hidden_state, cell_state):\n",
    "        \"\"\"\n",
    "        Decoding step:\n",
    "        1. Embed the current input token\n",
    "        2. Pass through LSTM with previous hidden/cell states\n",
    "        3. Project LSTM output to vocabulary size\n",
    "        4. Return prediction and updated states\n",
    "        \"\"\"\n",
    "        # input_token shape: [batch_size] - single token per batch item\n",
    "        \n",
    "        # Add sequence dimension for LSTM\n",
    "        input_token = input_token.unsqueeze(0)  # [1, batch_size]\n",
    "        \n",
    "        # Embed and apply dropout\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        # embedded shape: [1, batch_size, embedding_dim]\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_output, (new_hidden, new_cell) = self.lstm(embedded, (hidden_state, cell_state))\n",
    "        \n",
    "        # Remove sequence dimension and project to vocabulary\n",
    "        prediction = self.output_projection(lstm_output.squeeze(0))\n",
    "        # prediction shape: [batch_size, vocab_size]\n",
    "        \n",
    "        return prediction, new_hidden, new_cell\n",
    "\n",
    "print(\"Decoder implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySeq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    My complete sequence-to-sequence model\n",
    "    \n",
    "    This orchestrates the encoder and decoder to perform translation.\n",
    "    \n",
    "    Key design decision: Teacher forcing during training\n",
    "    - Sometimes use ground truth target tokens (teacher forcing)\n",
    "    - Sometimes use model's own predictions (preparing for inference)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        # Sanity checks - encoder and decoder must be compatible\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \"Hidden dimensions must match!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"Number of layers must match!\"\n",
    "    \n",
    "    def forward(self, source_seq, target_seq, teacher_forcing_ratio):\n",
    "        \"\"\"\n",
    "        My translation process:\n",
    "        1. Encode source sequence to get context\n",
    "        2. Initialize decoder with context\n",
    "        3. Generate target sequence one token at a time\n",
    "        4. Use teacher forcing probabilistically during training\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        batch_size = target_seq.shape[1]\n",
    "        target_length = target_seq.shape[0]\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # Tensor to store all predictions\n",
    "        predictions = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
    "        \n",
    "        # Step 1: Encode source sequence\n",
    "        hidden_context, cell_context = self.encoder(source_seq)\n",
    "        \n",
    "        # Step 2: Initialize decoder with <sos> token\n",
    "        decoder_input = target_seq[0, :]  # First token is <sos>\n",
    "        hidden_state, cell_state = hidden_context, cell_context\n",
    "        \n",
    "        # Step 3: Generate target sequence\n",
    "        for timestep in range(1, target_length):\n",
    "            # Predict next token\n",
    "            prediction, hidden_state, cell_state = self.decoder(\n",
    "                decoder_input, hidden_state, cell_state\n",
    "            )\n",
    "            \n",
    "            # Store prediction\n",
    "            predictions[timestep] = prediction\n",
    "            \n",
    "            # Decide on next input: teacher forcing or model prediction\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            if use_teacher_forcing:\n",
    "                # Use ground truth token\n",
    "                decoder_input = target_seq[timestep]\n",
    "            else:\n",
    "                # Use model's prediction\n",
    "                decoder_input = prediction.argmax(dim=1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "print(\"Complete Seq2Seq model implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters - my design choices\n",
    "input_vocab_size = len(de_vocab)    # German vocabulary size\n",
    "output_vocab_size = len(en_vocab)   # English vocabulary size\n",
    "embedding_dim = 256                 # Reasonable embedding size\n",
    "hidden_dim = 512                   # LSTM hidden state size\n",
    "n_layers = 2                       # Multi-layer for more capacity\n",
    "dropout_rate = 0.5                 # Regularization strength\n",
    "\n",
    "# Device setup - use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model components\n",
    "encoder = MyEncoder(\n",
    "    vocab_size=input_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "decoder = MyDecoder(\n",
    "    vocab_size=output_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Complete model\n",
    "model = MySeq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    My weight initialization strategy\n",
    "    \n",
    "    Using uniform distribution [-0.08, 0.08] as suggested in the original paper.\n",
    "    This helps with gradient flow in the early stages of training.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "            print(f\"Initialized {name}: {param.shape}\")\n",
    "\n",
    "# Initialize model weights\n",
    "model.apply(initialize_weights)\n",
    "print(\"\\nWeight initialization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training components\n",
    "optimizer = optim.Adam(model.parameters())  # Adam usually works well\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)  # Ignore padding tokens\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, clip_value, teacher_forcing_ratio, device):\n",
    "    \"\"\"\n",
    "    My training loop for one epoch\n",
    "    \n",
    "    Key aspects:\n",
    "    - Teacher forcing for stable training\n",
    "    - Gradient clipping for RNN stability\n",
    "    - Loss calculation excluding padding tokens\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Move data to device\n",
    "        src_seq = batch[\"de_ids\"].to(device)\n",
    "        tgt_seq = batch[\"en_ids\"].to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(src_seq, tgt_seq, teacher_forcing_ratio)\n",
    "        \n",
    "        # Prepare for loss calculation\n",
    "        # Skip first prediction (corresponds to <sos>)\n",
    "        predictions = predictions[1:].reshape(-1, predictions.shape[-1])\n",
    "        targets = tgt_seq[1:].reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    My evaluation function\n",
    "    \n",
    "    Key difference from training: no teacher forcing - model must rely\n",
    "    entirely on its own predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src_seq = batch[\"de_ids\"].to(device)\n",
    "            tgt_seq = batch[\"en_ids\"].to(device)\n",
    "            \n",
    "            # No teacher forcing during evaluation\n",
    "            predictions = model(src_seq, tgt_seq, teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Calculate loss\n",
    "            predictions = predictions[1:].reshape(-1, predictions.shape[-1])\n",
    "            targets = tgt_seq[1:].reshape(-1)\n",
    "            loss = criterion(predictions, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "print(\"Training functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "n_epochs = 10\n",
    "gradient_clip = 1.0\n",
    "teacher_forcing_ratio = 0.5  # 50% teacher forcing\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_save_path = 'my_seq2seq_model.pt'\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss = train_one_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        clip_value=gradient_clip,\n",
    "        teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Validation phase\n",
    "    valid_loss = evaluate_model(model, valid_loader, criterion, device)\n",
    "    \n",
    "    # Save best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved! Validation loss: {valid_loss:.3f}\")\n",
    "    \n",
    "    # Progress report\n",
    "    train_ppl = np.exp(train_loss)\n",
    "    valid_ppl = np.exp(valid_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.3f} | Train Perplexity: {train_ppl:.3f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.3f} | Valid Perplexity: {valid_ppl:.3f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence, model, de_nlp, en_nlp, de_vocab, en_vocab,\n",
    "    lower, sos_token, eos_token, device, max_length=25\n",
    "):\n",
    "    \"\"\"\n",
    "    My sentence translation function\n",
    "    \n",
    "    This is the real test - can the model translate arbitrary German sentences?\n",
    "    \n",
    "    Process:\n",
    "    1. Tokenize and preprocess German sentence\n",
    "    2. Encode to get context\n",
    "    3. Decode step by step until <eos> or max length\n",
    "    4. Convert indices back to English tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Preprocess input sentence\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in de_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = list(sentence)\n",
    "        \n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "        # Add special tokens\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        \n",
    "        # Convert to indices and tensor\n",
    "        indices = de_vocab.lookup_indices(tokens)\n",
    "        src_tensor = torch.LongTensor(indices).unsqueeze(-1).to(device)\n",
    "        \n",
    "        # Encode source sentence\n",
    "        hidden_context, cell_context = model.encoder(src_tensor)\n",
    "        \n",
    "        # Initialize decoding with <sos>\n",
    "        generated_indices = [en_vocab[sos_token]]\n",
    "        hidden_state, cell_state = hidden_context, cell_context\n",
    "        \n",
    "        # Generate translation token by token\n",
    "        for _ in range(max_length):\n",
    "            # Current input token\n",
    "            current_input = torch.LongTensor([generated_indices[-1]]).to(device)\n",
    "            \n",
    "            # Get next token prediction\n",
    "            prediction, hidden_state, cell_state = model.decoder(\n",
    "                current_input, hidden_state, cell_state\n",
    "            )\n",
    "            \n",
    "            # Get most likely next token\n",
    "            next_token_idx = prediction.argmax(-1).item()\n",
    "            generated_indices.append(next_token_idx)\n",
    "            \n",
    "            # Stop if we hit end-of-sequence\n",
    "            if next_token_idx == en_vocab[eos_token]:\n",
    "                break\n",
    "        \n",
    "        # Convert indices back to tokens\n",
    "        translated_tokens = en_vocab.lookup_tokens(generated_indices)\n",
    "        \n",
    "    return translated_tokens\n",
    "\n",
    "print(\"Translation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "print(\"Best model loaded\")\n",
    "\n",
    "# Test on a few examples\n",
    "test_sentences = [\n",
    "    \"Ein Mann sitzt auf einer Bank.\",  # A man sits on a bench\n",
    "    \"Zwei Kinder spielen im Park.\",    # Two children play in the park\n",
    "    \"Die Katze schläft auf dem Sofa.\", # The cat sleeps on the sofa\n",
    "]\n",
    "\n",
    "print(\"\\nTesting translation on sample sentences:\")\n",
    "for german_sentence in test_sentences:\n",
    "    translation = translate_sentence(\n",
    "        sentence=german_sentence,\n",
    "        model=model,\n",
    "        de_nlp=de_nlp,\n",
    "        en_nlp=en_nlp,\n",
    "        de_vocab=de_vocab,\n",
    "        en_vocab=en_vocab,\n",
    "        lower=lower,\n",
    "        sos_token=sos_token,\n",
    "        eos_token=eos_token,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Clean up translation (remove special tokens)\n",
    "    clean_translation = ' '.join(translation[1:-1])  # Remove <sos> and <eos>\n",
    "    \n",
    "    print(f\"German: {german_sentence}\")\n",
    "    print(f\"English: {clean_translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translations for entire test set\n",
    "print(\"Generating translations for test set...\")\n",
    "\n",
    "test_translations = []\n",
    "for example in tqdm.tqdm(test_data):\n",
    "    translation = translate_sentence(\n",
    "        sentence=example[\"de\"],\n",
    "        model=model,\n",
    "        de_nlp=de_nlp,\n",
    "        en_nlp=en_nlp,\n",
    "        de_vocab=de_vocab,\n",
    "        en_vocab=en_vocab,\n",
    "        lower=lower,\n",
    "        sos_token=sos_token,\n",
    "        eos_token=eos_token,\n",
    "        device=device\n",
    "    )\n",
    "    test_translations.append(translation)\n",
    "\n",
    "# Prepare data for BLEU calculation\n",
    "def create_tokenizer_function(nlp, lower):\n",
    "    def tokenize(text):\n",
    "        tokens = [token.text for token in nlp.tokenizer(text)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "    return tokenize\n",
    "\n",
    "tokenizer = create_tokenizer_function(en_nlp, lower)\n",
    "\n",
    "# Format predictions and references for BLEU\n",
    "predictions = [' '.join(translation[1:-1]) for translation in test_translations]  # Remove <sos>/<eos>\n",
    "references = [[example[\"en\"]] for example in test_data]  # BLEU expects list of references\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_results = bleu_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal BLEU Score: {bleu_results['bleu']:.4f}\")\n",
    "print(f\"Precision scores: {bleu_results['precisions']}\")\n",
    "print(f\"Brevity penalty: {bleu_results['brevity_penalty']:.4f}\")\n",
    "\n",
    "# Show a few example translations\n",
    "print(\"\\nSample translations:\")\n",
    "for i in range(3):\n",
    "    print(f\"German: {test_data[i]['de']}\")\n",
    "    print(f\"Expected: {test_data[i]['en']}\")\n",
    "    print(f\"My model: {predictions[i]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
