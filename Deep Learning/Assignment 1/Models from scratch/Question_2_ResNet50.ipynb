{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf618111",
   "metadata": {},
   "source": [
    "# ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e3a3548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports & Hyperparameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Device and hyperparameters\n",
    "device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "num_classes   = 100\n",
    "batch_size    = 128\n",
    "num_epochs    = 25\n",
    "learning_rate = 0.001\n",
    "momentum      = 0.9\n",
    "weight_decay  = 1e-4\n",
    "\n",
    "# CIFAR-100 normalization\n",
    "mean = [0.5071, 0.4867, 0.4408]\n",
    "std  = [0.2675, 0.2565, 0.2761]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761ea827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms & Loaders\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data',\n",
    "                                  train=True,\n",
    "                                  download=True,\n",
    "                                  transform=train_transform)\n",
    "val_dataset   = datasets.CIFAR100(root='./data',\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ebf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 Building Blocks\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bfbc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 Definition\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1     = nn.Conv2d(3, 64, kernel_size=7,\n",
    "                                   stride=2, padding=3, bias=False)\n",
    "        self.bn1       = nn.BatchNorm2d(64)\n",
    "        self.relu      = nn.ReLU(inplace=True)\n",
    "        self.maxpool   = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # layers configuration: [3, 4, 6, 3]\n",
    "        self.layer1 = self._make_layer(64,  3)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc      = nn.Linear(512 * Bottleneck.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * Bottleneck.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes,\n",
    "                          planes * Bottleneck.expansion,\n",
    "                          kernel_size=1,\n",
    "                          stride=stride,\n",
    "                          bias=False),\n",
    "                nn.BatchNorm2d(planes * Bottleneck.expansion),\n",
    "            )\n",
    "        layers = [Bottleneck(self.in_planes, planes, stride, downsample)]\n",
    "        self.in_planes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd0c3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model & Save Initial Filters\n",
    "\n",
    "model = ResNet50(num_classes=num_classes).to(device)\n",
    "initial_conv1 = model.conv1.weight.data.clone().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19d26861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, Optimizer & Scheduler\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=learning_rate,\n",
    "                      momentum=momentum,\n",
    "                      weight_decay=weight_decay)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99873b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25  Train Loss 4.2029  Acc 0.0635  Val   Loss 3.8743  Acc 0.1052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25  Train Loss 3.6751  Acc 0.1351  Val   Loss 3.5107  Acc 0.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25  Train Loss 3.4063  Acc 0.1814  Val   Loss 3.2946  Acc 0.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25  Train Loss 3.1817  Acc 0.2231  Val   Loss 3.2313  Acc 0.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25  Train Loss 2.9783  Acc 0.2629  Val   Loss 2.9231  Acc 0.2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25  Train Loss 2.8014  Acc 0.2944  Val   Loss 2.8792  Acc 0.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 23\u001b[0m run_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m preds \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m run_corr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training & Validation Loop with Progress Bars\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,   val_accs   = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # -----------------\n",
    "    # Training\n",
    "    # -----------------\n",
    "    model.train()\n",
    "    run_loss, run_corr, run_tot = 0.0, 0, 0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [train]\", leave=False)\n",
    "    for x, y in train_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        run_loss += loss.item() * x.size(0)\n",
    "        preds = out.argmax(1)\n",
    "        run_corr += (preds == y).sum().item()\n",
    "        run_tot  += y.size(0)\n",
    "\n",
    "        train_bar.set_postfix({\n",
    "            'loss': f\"{run_loss/run_tot:.4f}\",\n",
    "            'acc':  f\"{run_corr/run_tot:.4f}\"\n",
    "        })\n",
    "\n",
    "    train_losses.append(run_loss / run_tot)\n",
    "    train_accs.append(run_corr / run_tot)\n",
    "\n",
    "    # -----------------\n",
    "    # Validation\n",
    "    # -----------------\n",
    "    model.eval()\n",
    "    run_loss, run_corr, run_tot = 0.0, 0, 0\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [val]  \", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_bar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "\n",
    "            run_loss += loss.item() * x.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            run_corr += (preds == y).sum().item()\n",
    "            run_tot  += y.size(0)\n",
    "\n",
    "            val_bar.set_postfix({\n",
    "                'loss': f\"{run_loss/run_tot:.4f}\",\n",
    "                'acc':  f\"{run_corr/run_tot:.4f}\"\n",
    "            })\n",
    "\n",
    "    val_losses.append(run_loss / run_tot)\n",
    "    val_accs.append(run_corr / run_tot)\n",
    "\n",
    "    # -----------------\n",
    "    # Scheduler Step\n",
    "    # -----------------\n",
    "    scheduler.step()\n",
    "\n",
    "    # -----------------\n",
    "    # Epoch Summary\n",
    "    # -----------------\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs}  \"\n",
    "        f\"Train Loss {train_losses[-1]:.4f}  Acc {train_accs[-1]:.4f}  \"\n",
    "        f\"Val   Loss {val_losses[-1]:.4f}  Acc {val_accs[-1]:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94040fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss & Accuracy\n",
    "\n",
    "epochs = np.arange(1, num_epochs+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses,   label='Val Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_accs, label='Train Acc')\n",
    "plt.plot(epochs, val_accs,   label='Val Acc')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.title('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e64d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Update Analysis & Filter Visualization\n",
    "\n",
    "final_conv1 = model.conv1.weight.data.clone().cpu()\n",
    "l2_change   = torch.norm(final_conv1 - initial_conv1)\n",
    "print(f\"Conv1 L2 weight change: {l2_change:.4f}\")\n",
    "\n",
    "def show_filters(filters, title):\n",
    "    n = 2  # first two filters\n",
    "    fig, axs = plt.subplots(1, n, figsize=(n*3, 3))\n",
    "    for i in range(n):\n",
    "        f = filters[i]\n",
    "        fmin, fmax = f.min(), f.max()\n",
    "        img = (f - fmin)/(fmax - fmin)\n",
    "        img = img.permute(1,2,0).numpy()\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off')\n",
    "        axs[i].set_title(f\"{title} #{i}\")\n",
    "    plt.show()\n",
    "\n",
    "show_filters(initial_conv1, \"Before Training\")\n",
    "show_filters(final_conv1,   \"After Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6269f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing & Metrics\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        preds = out.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "mlcms = multilabel_confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "rows = []\n",
    "for cls in range(num_classes):\n",
    "    tn, fp, fn, tp = mlcms[cls].ravel()\n",
    "    f1 = (2*tp)/(2*tp + fp + fn) if (2*tp + fp + fn)>0 else 0.0\n",
    "    rows.append([cls, tp, tn, fp, fn, f1])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=['class','tp','tn','fp','fn','f1_score'])\n",
    "display(df)\n",
    "df.to_csv('cifar100_metrics.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
